{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdEu4aFr7EwI"
   },
   "source": [
    "## Argument Mining Practical Session\n",
    "\n",
    "To use this on Colab, run the following cells. (Authentication required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1PAtUQQ4XRp",
    "outputId": "11fcf06b-ca35-4a67-aaf3-6d569ab1e1e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtmnvrYp5iBe",
    "outputId": "718e8314-4ac8-4107-a101-c7949e668018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/share\n"
     ]
    }
   ],
   "source": [
    "%cd gdrive/MyDrive/share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBaZ0hMe7Tq2"
   },
   "source": [
    "Now we are cloning the Dataset into our Google Drive (in the folder specified earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPSe0d2c54uT",
    "outputId": "a75da12f-885e-4ec9-8711-144eb3b00a3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'abstrct'...\n",
      "remote: Enumerating objects: 1369, done.\u001b[K\n",
      "remote: Counting objects: 100% (1369/1369), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1365/1365), done.\u001b[K\n",
      "remote: Total 1369 (delta 5), reused 1353 (delta 0), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (1369/1369), 1.11 MiB | 3.18 MiB/s, done.\n",
      "Resolving deltas: 100% (5/5), done.\n",
      "Checking out files: 100% (1406/1406), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://gitlab.com/tomaye/abstrct.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-K1yBleM6VAq",
    "outputId": "75908505-d312-4fa3-a0f3-af02253a30c1"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_-bwMTC7bwg"
   },
   "source": [
    "We're good to go.\n",
    "\n",
    "## First Part\n",
    "Let's begin with some imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYxqdaIK7CaN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ArgumentMining_AI4HEALTH.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
